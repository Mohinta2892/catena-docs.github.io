{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CATENA Documentation","text":"Talk to us: Discord <p>Welcome to CATENA: an end-to-end, developer-friendly pipeline for large-scale connectomics. If you are curious why do we call it CATENA, read here.</p> <p>CATENA brings together neuron segmentation (LSDs), synapse detection (Synful), microtubule tracking, neurotransmitter classification, mitochondria segmentation, EM masking, and visualization tools.</p> <p>Use the sidebar to browse each module. If you're new, start with Getting Started.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#install","title":"Install","text":"<ul> <li>Clone the repo</li> <li>Create the relevant conda env or use the Docker for the module you want to run</li> <li>See each module's Overview page for exact steps</li> </ul> Important <p>The modules are independent of each other. You can run any module on its own. Please follow the README.md file for each module in the GitHub. The README.md file contains most up-to-date instructions.</p>"},{"location":"getting-started/#our-modules-for-connectome-mapping-enrichment-and-visualization","title":"Our Modules for Connectome mapping, enrichment and visualization","text":"<ul> <li>EM Mask Generation. This is a module to generate brain/resin masks from 3D EM volumes.</li> <li>Local Shape Descriptors (Neuron Segmentation)</li> <li>Synapse Detection<ul> <li>Synful</li> <li>SimpSyn</li> </ul> </li> <li>Mitochondria Segmentation</li> <li>Neurotransmitter Classification</li> <li>Proofreading</li> <li>Visualization (Napari, Neuroglancer)</li> </ul>"},{"location":"why_name_it_catena/","title":"The story behind the name CATENA","text":"<p>Firstly, I should clarify that CATENA is not an acronym. So CATENA can be written as Catena or catena.</p> <p>The story behind naming this package is actually very simple and maybe a little silly.</p> <p>When I ventured into connectomics after two years of working on Magnetic Resonance Imaging at the Queen Square Institute of Neurology, UCL, UK, I was quite stunned by the fragmented software ecosystem in the connectomics world. There were so many good tools, algorithms, and packages, but most were niche. However, a connectome, to me, meant not only that the core neurons and synapses were identified so that a connectivity matrix or a graph could be built, but also that we had other features identified such as cell-types, neurotransmitters at synaptic locations, and even mitochondria.</p> <p>Existing software did all of these things, but never under one umbrella. Different groups optimized different parts, which meant that one had to first find which was the best model to do a given task and then learn its software dependencies. A common problem was also that many of these software packages were old and unmaintained; what you may have read in the paper could no longer be reproduced in code.</p> <p>All of this led me to believe that I could create a package that, while very much standing on the shoulders of giants, gives users (connectome mappers) a one-stop link where they can explore different aspects of both mapping a core connectome and enriching the same connectome with further information. I wanted to bind or <code>chain</code> the popular tools (and even invent a few of my own) under one hood that I would later call Catena. </p> <p>Catena also literally means a chain of connected ideas or objects.</p>"},{"location":"modules/em-mask-generation/","title":"EM Mask Generation","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts in the GitHub repository for the most up-to-date instructions.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/em-mask-generation/#what-is-the-em-mask-generation-module","title":"What is the EM Mask Generation module?","text":"<p>In this module, we generate EM masks that label which parts of a volume are brain and which parts are non-brain tissue. Think of it as a tissue-labelling step: we want a clean mask that says \"this voxel belongs to brain or ventral nerve cord\" vs \"this is resin, trachea, or something we don\u2019t plan to segment\".</p> <p>For now, the examples are based on Drosophila larva FIBSEM volumes (downsampled), but the ideas are general.</p> <p>We work with two classes:</p> <ul> <li>Brain \u2013 any tissue that forms the brain or ventral nerve cord.  </li> <li>Non-brain \u2013 resin, tracheal membranes, cutting artefacts, or any regions of the volume we do not want to segment further.</li> </ul>"},{"location":"modules/em-mask-generation/#why-do-we-need-these-masks","title":"Why do we need these masks?","text":"<p>Most downstream steps in Catena (neuron segmentation, synapses, mitochondria, neurotransmitters, etc) only care about brain tissue. Resin and other non-brain structures just:</p> <ul> <li>waste GPU time and memory,</li> <li>could introduce false positives (e.g. \u201cneuron segments\u201d on resin),</li> <li>and make evaluation and proofreading more painful.</li> </ul> <p>A good brain vs non-brain mask lets us:</p> <ul> <li>restrict all heavy models and post-processing to relevant voxels only,</li> <li>standardise crops and RoIs across modules,</li> <li>and quickly visualise which parts of a huge EM volume are actually \u201cin play\u201d for connectomics.</li> </ul> <p>In short: these masks are a cheap but powerful way to ignore what we don\u2019t care about.</p>"},{"location":"modules/em-mask-generation/#how-do-we-generate-these-masks","title":"How do we generate these masks?","text":"<p>We provide two ways to obtain a brain vs non-brain mask.</p>"},{"location":"modules/em-mask-generation/#1-conventional-computer-vision","title":"1. Conventional computer vision","text":"<p>As a lightweight, dependency-free baseline we start with classical CV:</p> <ul> <li>apply a Difference of Gaussians (DoG) filter, and  </li> <li>run a local texture / structure analysis to build masks.</li> </ul> <p>The pure DoG masks are not great on their own, but the texture-based masking can give a decent first separation of \u201cbackground resin\u201d from \u201cbiological tissue\u201d. However, this approach cannot easily distinguish other non-brain structures such as tracheal membranes. It is a good starting point, but not the whole story.</p> <p>Warning</p> <p>The DoG/texture-based masks require manual finetuning of hyper-parameters such as <code>block_size</code>, <code>texture_sigma</code>, <code>edge_sigma</code> and <code>minimum_region_size</code>. Check design notes for more details</p>"},{"location":"modules/em-mask-generation/#2-machine-learning","title":"2. Machine learning","text":"<p>To go beyond that, we train a very simple 3D U-Net on a downsampled EM volume with semantic labels for brain and non-brain:</p> <ul> <li>implemented with MONAI,</li> <li>minimal data augmentation to encourage some generalisation to other EM datasets,</li> <li>trained in pixel space (we do not encode voxel size explicitly).</li> </ul> <p>At inference, this model generalises well (though of course not perfectly) to completely unseen EM volumes. The predicted masks already do a good job at separating brain tissue from everything else and are much better at handling structures like tracheal membranes.</p> <p>A post-processing step should follow after inference to:</p> <ul> <li>fill small gaps,</li> <li>slightly grow/erode the mask, and  </li> <li>make sure it overlays cleanly on the EM volume.</li> </ul> <p>An example post-processing script is provided in this module.</p>"},{"location":"modules/em-mask-generation/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/em-mask-generation/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/em-mask-generation/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/em-mask-generation/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/em-mask-generation/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/em-mask-generation/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p> <p>Inputs</p> <p>volume 3D EM volume (or 2D slice stack) in which we want to separate \u201cbrain-like texture\u201d from background resin / junk.</p> <p>block_size Size of the local window (in pixels/voxels) used for computing texture statistics.</p> <p>Smaller blocks \u2192 more detailed, sensitive to small variations but also to noise.</p> <p>Larger blocks \u2192 smoother, more robust, but can miss tiny islands of tissue. In practice, 64 is a good starting point for downsampled FIBSEM; scale it roughly with your in-plane resolution.</p> <p>texture_sigma Standard deviation of the Gaussian used in the texture filter (e.g. for smoothing features such as local variance or structure tensor).</p> <p>Lower values \u2192 focus on very fine texture details (good for high-res, noisy EM).</p> <p>Higher values \u2192 capture coarser patterns, ignore tiny fluctuations. If the mask looks very noisy, try increasing this slightly.</p> <p>edge_sigma Standard deviation of the Gaussian used in the edge / gradient filter. This controls the spatial scale of edges that contribute to the mask.</p> <p>Small values (e.g. 0.5) \u2192 respond to fine edges and thin structures.</p> <p>Larger values \u2192 emphasise broader transitions, ignore tiny speckles. If you\u2019re missing thin tissue at boundaries, decrease this a bit; if you\u2019re getting a lot of edge noise, increase it.</p> <p>min_region_size Minimum size (in pixels/voxels) of a connected component to keep in the final mask. Everything smaller is dropped as noise.</p> <p>Increase this if you see lots of tiny specks of \u201ctissue\u201d in resin.</p> <p>Decrease it if you actually care about very small islands of brain tissue. This should be tuned relative to your image size and downsampling; 250 is a reasonable starting point for typical subvolumes.</p> <p>Outputs</p> <p>mask Binary mask (same shape as volume): 1 for \u201ctexture consistent with tissue we care about\u201d, 0 for background / non-brain.</p> <p>confidence Float array with a continuous score per pixel/voxel (e.g. how strongly the local texture looks like \u201cbrain\u201d). This is useful if you want to:</p> <p>tweak the threshold later without recomputing everything,</p> <p>visualise \u201csoft\u201d tissue likelihood,</p> <p>or combine this with other signals (e.g. learned model logits) before making a hard mask.</p>"},{"location":"modules/em-mask-generation/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/em-mask-generation/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"},{"location":"modules/local-shape-descriptors/","title":"Local Shape Descriptors","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: Please follow module's README and scripts in GitHub for most up-to-date documentation.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul> <p>Note Our automated methods for connectome mapping will use Electron Microscopy (EM) datasets.</p>"},{"location":"modules/local-shape-descriptors/#what-is-neuron-segmentation","title":"What is neuron segmentation?","text":"<p>In connectomics -- the art of mapping neural wiring diagrams --  we want to trace every neuron throughout a dataset (here Electron Microscopy images or volumes of real brains). These neurons are long, wiggly structures that snake across regions, branch, loop back, and find their friends (other neurons) to make synaptic connections.</p> <p>Traditionally, people did this by hand using CATMAID-like toolkits (The Collaborative Annotation Toolkit for Massive Amounts of Image Data). This is slow, exhausting, and extremely hard to scale to modern datasets that can span terabytes to petabytes. Neuron segmentation is about automating this tracing process so we can reliably reconstruct whole connectomes without an army of human annotators.</p>"},{"location":"modules/local-shape-descriptors/#what-are-local-shape-descriptors","title":"What are Local Shape Descriptors?","text":"<p>Local Shape Descriptors (LSDs) introduce an auxiliary learning task aimed at improving neuron segmentation within electron microscopy volumes. These descriptors are employed alongside conventional voxel-wise direct neighbor affinities to enhance neuron boundary detection. By capturing key local statistics of neurons, such as diameter, elongation, and direction, LSDs significantly refine segmentation accuracy. Comparative studies across a variety of specimens, imaging techniques, and resolutions reveal that incorporating LSDs consistently elevates the performance of affinity-based segmentation methods. This approach not only matches the effectiveness of current state-of-the-art neuron segmentation techniques but also offers a leap in computational efficiency, making it indispensable for processing the extensive datasets expected in future connectomics research.</p> <ul> <li>Read the paper here: Sheridan et al., Nature Methods, 2023</li> <li>Read the original LSD blogpost here</li> </ul> <p>LSDs architecture at a glance (multi-task U-Net) </p> <p> </p> <p>Note These are supervised ML models, hence you need ground truth. Primary tests reveal 40 cubic microns of densely segmented volumes is good to begin with.</p>"},{"location":"modules/local-shape-descriptors/#getting-started","title":"Getting started","text":"<p>Read these:</p> <ul> <li>System Requirements</li> <li>Installation instructions: Docker, Conda</li> <li>Dataset preparation</li> </ul>"},{"location":"modules/local-shape-descriptors/#usage-instructions","title":"Usage instructions","text":"<p>IMPORTANT If you want to only run inference with pretrained model, follow steps.</p>"},{"location":"modules/local-shape-descriptors/#semantic-segmentation-to-get-the-affinity-maps","title":"Semantic Segmentation to get the affinity maps","text":"1. Understand and modify as needed the config.py  For training models   `config.py` contains `SYSTEM`, `DATA`, `PREPROCESS`, `TRAIN`, `MODEL_ISO` (for isotropic datesets) and `MODEL_ANISO` (for anisotropic datasets). Most of these configurations and hyper-parameters have been populated with default used during experiments. You may want to modify them to suit your needs. Please look at the commented text adjacent to the hyper-params set to get an idea of what they are.  Separate `config.py` files for public datasets like CREMI, SNEMI, ZEBRAFINCH are provided.   For running inference with trained models   `config_predict.py` should be used to run affinity prediction. All configurations set in the file should be automatically picked up by `predicter.py` or `super_predicter_daisy.py`. Ensure you set the same architectural hyper-parameters under `MODEL_ISO` OR `MODEL_ANISO` for pytorch to load the weights correctly. Also, ensure you put the data in the correct path inside a `test` folder, and pass the correct `model checkpoint`.   2. Train models with trainer.py  For training models    Set the hyper-params in the `config.py` file and then run:  <pre><code>python trainer.py -c config_cremi.py\n</code></pre>  Note: When a config file is not passed, the default is `config.py`.   3.1 Run affinity predictions as a single process with predicter.py   You can place as many datasets in the `test` folder of your `BRAIN_VOLUME` as you want. Each will be processed but sequentially.  Download **pretrained** models from [here](https://www.dropbox.com/scl/fo/uxmoj3v6i8mos6lwjjvio/h?rlkey=w10iia8rd8alkx3i67u88w0er&amp;dl=0). These models have mostly been trained with default architectural params. We will share more details sooner.  Please modify `config_predict.py` to match your `config.py` used during training. Check **above** for details.   Run prediction  <pre><code>python predicter.py\n</code></pre>  Note: `predicter.py` does not accept a `config.py` args yet! Hence, all changes must be made in `config_predict.py` as this is default.   3.2 Run affinity predictions blockwise multiprocessing with super_predicter_daisy.py  &gt; **WARNING**  &gt; THIS HAS ONLY BEEN TESTED WITH 3D VOLUMES AND USE `SBATCH` FOR SLURM TO SPAWN ACROSS MULTIPLE CARDS AND A VERY LARGE DATASET. &gt; YOU CAN CHANGE IT TO A LOCAL `SUBPROCESS` RUN. WE WILL ALLOW A `ARGS` INPUT FOR THIS SOON.  You can place as many datasets in the `test` folder of your `BRAIN_VOLUME` as you want. Each will be processed but sequentially USING MULTIPLE-WORKERS, which makes the predictions faster.  Download **pretrained** models from [here](https://www.dropbox.com/scl/fo/uxmoj3v6i8mos6lwjjvio/h?rlkey=w10iia8rd8alkx3i67u88w0er&amp;dl=0). These models have mostly been trained with default architectural params. We will share more details sooner.  Please modify `config_predict.py` to match your `config.py` used during training. Check **above** for details.   Run prediction parallely with Daisy task scheduling  <pre><code>python super_predicter_daisy.py -c config/config_predict_{brain_volume}.py\n</code></pre>  Run `super_predicter_daisy_chunkskipping.py` to skip boundary blocks if they don't need to be segmented  <pre><code>python super_predicter_daisy_chunkskipping.py -c config/config_predict_{brain_volume}.py\n</code></pre>"},{"location":"modules/local-shape-descriptors/#instance-segmentation-from-predicted-affinities","title":"Instance Segmentation from predicted affinities","text":"<p>IMPORTANT Output segmentations are saved in the same output zarr under <code>lsd_outputs</code> containing <code>volumes/pred_affs</code>. Agglomeration thresholds are appended to dataset names: <code>volumes/segmentation_055</code></p> 4.1 Extract supervoxels and agglomerate for small ROIs with instance_segmenter.py  &gt; **WARNING**  &gt; This script should be used with volumes that fit into memory. Predicted affinities are cast as before watershedding float32, so you should have enough RAM.  You must keep the output affinities under `lsd_outputs` for `instance_segmenter.py` to pick them up. Edit data paths in `config_predict.py`. Watershed and agglomeration will be run sequentially on all output *zarr* files that contain `volumes/pred_affs`.   Run watershed and agglomeration  <pre><code>python instance_segmenter.py\n</code></pre> 4.2 Extract supervoxels chunk-wise from large volumes with 02_extract_fragments_blockwise.py  &gt; **IMPORTANT**  &gt; Install [MongoDB](https://www.mongodb.com/docs/manual/installation/) before you begin.  &gt; Ensure you have `pymongo~=4.3.3` and `daisy~=1.0`  &gt; **WARNING**  &gt; `db_host = \"localhost:27017\"` and `db_name = \"lsd_parallel_fragments\"` are hardcoded as these in the script. Yet to be supported via `config_predict.py`. `collection_name` would be auto set to the name of your zarr file.   Run watershed with daisy chunk-wise <pre><code>python 02_extract_fragments_blockwise.py\n</code></pre> **NB: 02_extract_fragments_blockwise.py calls [02_extract_fragments_worker.py](engine/post/02_extract_fragments_worker.py)**   5. Agglomerate supervoxels of large volumes chunk-wise with 03_agglomerate_blockwise.py  &gt; **WARNING**  &gt; This cannot be run if `02_extract_fragments_blockwise.py` has not been run.   Run agglomeration with daisy chunk-wise <pre><code>python 03_agglomerate_blockwise.py\n</code></pre> **NB: 03_agglomerate_blockwise.py calls [03_agglomerate_worker.py](engine/post/03_agglomerate_worker.py)**"},{"location":"modules/local-shape-descriptors/#final-steps-to-extract-final-segmentation-for-large-volumes","title":"Final steps to extract final segmentation for LARGE volumes","text":"6. Finding all segments and saving them as Look-Up-Tables (LUTs) 04_find_segments_full.py  &gt; **WARNING**  &gt; This cannot be run if `03_agglomerate_blockwise.py` has not been run.  &gt; **Don't forget to pass `daisy_logs/{filename}_fragments/config_0.yaml` from your daisy_logs folder auto-created under `catena/local_shape_descriptors`.**  &gt; Output LUTs are saved under `lsd_outputs`   Create a LUT file    Note: `hist_quant_50` must exist in the db as a collection. If not, check your config file to find at what is the starting value of `_C.INS_SEGMENT.THRESHOLDS`, that is your `hist_quant_{value_in_decimals * 100}`. Example: for `_C.INS_SEGMENT.THRESHOLDS = np.arange(0.35, 0.80, 0.05).tolist()`, you should pass `-mf hist_quant_35`.  <pre><code>python 04_find_segments_full.py -c daisy_logs/{filename}_fragments/config_0.yaml -mf hist_quant_50 -th 0.7\n</code></pre>  Extracting a final segmentation 05_extract_segmentation_from_lut.py  &gt; **WARNING**  &gt; This cannot be run if `04_find_segments_full.py` has not been run.  &gt; **Don't forget to pass `daisy_logs/{filename}_pred_affs/config_0.yaml` from your daisy_logs folder auto-created under `catena/local_shape_descriptors`.**  &gt; Final segmentations are saved in the zarr under `lsd_outputs`.   Extract Segments from LUT    Note: `hist_quant_50` must exist in the db as a collection. If not, check your config file to find at what is the starting value of `_C.INS_SEGMENT.THRESHOLDS`, that is your `hist_quant_{value_in_decimals * 100}`. Example: for `_C.INS_SEGMENT.THRESHOLDS = np.arange(0.35, 0.80, 0.05).tolist()`, you should pass `-mf hist_quant_35`. <pre><code>python 05_extract_segmentation_from_lut.py -c daisy_logs/{filename}_fragments/config_0.yaml -mf hist_quant_50 -th 0.7\n</code></pre> <p>NOTE Small Rois can be proofread with Napari-based Seg2Link.</p>"},{"location":"modules/local-shape-descriptors/#performance-of-lsds-on-held-out-in-distribution-and-out-of-distribution-datasets","title":"Performance of LSDs on held-out (in-distribution) and out-of-distribution datasets","text":"<ol> <li>Performance on Octo (unseen data): A Multi-task (MT) LSD model trained solely on Hemibrain (resolution: <code>8nm^3</code>, FIBSEM; <code>~40</code> microns) dataset on Octo (resolution: <code>8nm^3</code>, FIBSEM; <code>20 x 200 x 200</code> Roi in zyx) achieves <code>~0.05</code> Adapted Rand Error for a segmentation for agglomeration thresholds <code>50%-60%</code>.</li> </ol> <ul> <li>3D Neuroglancer Snapshot of Selected Segmentations</li> </ul>"},{"location":"modules/local-shape-descriptors/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/local-shape-descriptors/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>We want to robustly segment neurons in large Electron Microscopy (EM) volumes, and do so in a way that:</p> <ul> <li>scales from \u201ctoy\u201d ROIs (small subvolumes) to multi-terabyte datasets,</li> <li>works across different microscopes, preparations, and organisms,</li> <li>stays reproducible and re-runnable on normal lab infrastructure (HPC / workstations),</li> <li>plugs cleanly into downstream connectomics steps (synapses, mitochondria, neurotransmitters, proofreading).</li> </ul> <p>In practical terms: given raw EM, we want a bottom-up pipeline that predicts local fields (affinities + LSDs) and then turns those into instances via watershed and agglomeration.</p>"},{"location":"modules/local-shape-descriptors/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Affinities-only U-Net </li> <li>Pros: simpler targets, lighter bookkeeping, fewer outputs. We support this within our current neuron segmentation setup. You can forego generating LSDs and train an affinity-only model.</li> <li> <p>Cons: less shape-awareness, more brittle in thin processes, more sensitive to hyperparameters during agglomeration. LSDs often show consistently better or more stable behaviour for the same backbone.</p> </li> <li> <p>Boundary-only / membrane U-Nets (torchEM / BiaPy-style) </p> </li> <li>Pros: conceptually straightforward (predict a membrane probability map), widely used, and well-supported in libraries like torchEM and BiaPy. Easy to plug into standard watershed + agglomeration toolchains from <code>scikit-image</code> (even be inspired from the PlantSeg) and to reuse existing code/configs.  </li> <li> <p>Cons: all the instance information is pushed into a single boundary channel (could be expanded to use Contours and Distance). This tends to make training and post-processing more sensitive to class imbalance, noise, and local artefacts; and you often need dataset-specific tuning of thresholds and post-processing to get robust neuron instances. LSDs explicitly inject local shape statistics, which we found to stabilise training and make the affinities less brittle.</p> </li> <li> <p>Flood-Filling Networks (FFNs) </p> </li> <li>Pros: strong instance segmentation story, conceptually close to how humans might trace neurons.  </li> <li> <p>Cons: heavier runtime, more complex code-paths, harder to integrate with our Zarr/Daisy/Gunpowder stack, and less friendly to small teams wanting to run and extend the pipeline. </p> </li> <li> <p>Different storage formats (HDF5-only, N5, TIFF) </p> </li> <li>Pros: sometimes simpler to inspect (TIFF), widely used (HDF5), compatible with some existing tools. We do use N5 as Zarr allows loading N5s and we also use HDF5s, but predominantly for synapse detection. </li> <li>Cons: TIFFs can be less ergonomic for distributed processing at scale; more friction with our chosen ecosystem; harder to maintain a single, consistent story across modules.</li> </ul>"},{"location":"modules/local-shape-descriptors/design-choices/#decision","title":"Decision","text":"<p>We standardised on the following:</p> <ul> <li>Model family: multi-task U-Nets that jointly predict LSDs and affinities (and optionally other channels such as mitochondria).</li> <li>I/O format: Zarr, with standardised dataset names under <code>volumes/raw</code> for EM; <code>volumes/labels/neuron_ids</code> for neuron labels. Note: We follow the CREMI format for laying out our data.</li> <li>Scaling: Gunpowder for data loading/augmentation + Daisy for block-wise prediction and large-volume instance segmentation.</li> <li>Downstream segmentation: watershed + agglomeration on affinities, using LSDs as an auxiliary signal during training to make those affinities \u201cbetter behaved\u201d.</li> </ul> <p>This gives us:</p> <ul> <li>a reasonably small and understandable codebase,</li> <li>a single storage + processing story that generalises to other Catena modules,</li> <li>and a path that remains open, reproducible, and modifiable by other labs.</li> </ul>"},{"location":"modules/local-shape-descriptors/design-choices/#why-gunpowder","title":"Why Gunpowder?","text":"<p>We build the training and augmentation pipeline on top of Gunpowder.</p> <p>One key reason is that Gunpowder represents every array with an explicit voxel size and ROI. All crops, fields-of-view, and augmentations are defined in this voxel-resolution space and then applied relative to the data\u2019s native resolution. In practice, this means the same pipeline configuration naturally adapts across datasets with different voxel sizes: a \u201ccertain amount of context\u201d really means a certain amount of physical context, not just an arbitrary number of pixels.</p> <p>This makes the training setup more robust and resolution-aware than pixel-based scripts.</p>"},{"location":"modules/local-shape-descriptors/design-choices/#trade-offs","title":"Trade-offs","text":"<ul> <li> <p>More targets to generate   LSDs require additional pre-processing to create per-voxel shape descriptors. Though this happens on-the-fly as part of the data augmentation pipeline, it adds a bit of complexity and hence the training is overall slower than you would see in a affinity-only model.</p> </li> <li> <p>Slightly larger models / outputs   Predicting both LSDs and affinities means more output channels and slightly higher memory usage than a pure-affinity U-Net.</p> </li> <li> <p>Zarr-first means conversion upfront   If you start from HDF5 or raw TIFFs, there is an upfront conversion step. For tiny test volumes this can feel like overkill, but it pays off once you move beyond toy/ROI scales.</p> </li> <li> <p>Watershed + agglomeration stack   Instance segmentation is a multi-step process (prediction \u2192 fragments \u2192 agglomeration \u2192 LUT \u2192 final segmentation), and some steps rely on MongoDB and batch jobs. This is more moving parts than a toy example using small subvolumes, but it\u2019s also what lets us scale to big volumes and explore different thresholds without re-running everything.</p> </li> </ul>"},{"location":"modules/local-shape-descriptors/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Some relevant components in this module:</p> <ul> <li>Data preparation</li> <li><code>create_dir_organisation.py</code>: builds the directory structure for 2D/3D, train/test, and domains.</li> <li> <p><code>hdf_to_zarr.py</code>: converts HDF5 datasets into Zarr and adds required label/mask datasets. We always use zarr during model training and inference.</p> </li> <li> <p>Config modification and management</p> </li> <li> <p><code>config_{datavolume}.py</code>: adapt and modify a config file. These shared config files here are examples that we ourselves have used for training our models.   Each config is named based on the dataset it used during training. Input shapes, downsampling factors and kernels are guided by the EM resolution. In lay terms, this means that anisotropic models (on anisotropic datasets) are trained differently to how isotropic models.  Generally, the downsampling is adjusted such that the data eventually becomes isotropic as it passes through the model's convolutional layers till the bottleneck layer. Augmentations are also applied anisotropically for anisotropic datasets.</p> </li> <li> <p>Models</p> </li> <li> <p><code>MtlsdModel</code>, <code>MtlsdMitoModel</code>, <code>LsdModel</code>, <code>AffModel</code> under <code>local_shape_descriptors/models</code>: PyTorch implementations of multi-task U-Nets for LSDs, affinities, and (optionally) mitochondria. We strongly recommend using MTLSD model </p> </li> <li> <p>Training / prediction</p> </li> <li><code>trainer.py</code>: multi-task LSD + affinity training using Gunpowder.</li> <li><code>predicter.py</code>: single-process prediction on smaller volumes.</li> <li> <p><code>super_predicter_daisy.py</code>: Daisy-driven, block-wise prediction for large volumes.</p> </li> <li> <p>Instance segmentation</p> </li> <li><code>instance_segmenter.py</code>: watershed + agglomeration for small-ish volumes.</li> <li><code>02_extract_fragments_blockwise.py</code>, <code>03_agglomerate_blockwise.py</code>, <code>04_find_segments_full.py</code>, <code>05_extract_segmentation_from_lut.py</code>: Daisy + MongoDB-based large-volume fragment extraction, agglomeration, LUT generation, and final segmentation writing.</li> </ul> <p>Outputs are also written into Zarrs.</p>"},{"location":"modules/local-shape-descriptors/design-choices/#operational-guidance","title":"Operational Guidance","text":"<ul> <li>Use Zarr from the beginning if you know the dataset will grow beyond small subvolumes (ROIs).  </li> <li>For quick experiments / small ROIs, <code>predicter.py</code> + <code>instance_segmenter.py</code> are the path of least resistance.  </li> <li>For large volumes, always:</li> <li>Convert to Zarr,</li> <li>Use <code>super_predicter_daisy_{chunkskipping}.py</code> for affinities (and LSDs),</li> <li> <p>Run the block-wise watershed + agglomeration scripts.</p> </li> <li> <p>Start with the provided configs for public datasets (CREMI, SNEMI, etc.) and only deviate once you have a baseline.</p> </li> </ul>"},{"location":"modules/local-shape-descriptors/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<ul> <li>Better, more automatic hyperparameter selection for agglomeration thresholds.</li> <li>Tighter integration with other Catena modules (e.g. reusing LSDs as features for other segmentation tasks). We can currently couple mitochondria segmentation and neuron segmentation in the same LSDs framework.</li> <li>Exploring more <code>top-down</code> approaches on top of LSDs (e.g. graph-based refinement, morphology-aware merges/splits).</li> <li>Support for alternative storage backends (CloudVolume / NGFF) while keeping (OME)-Zarr as the main path.</li> </ul>"},{"location":"modules/local-shape-descriptors/system_requirements/","title":"System Requirements","text":"<p>Operating System: Ubuntu 20.4 or above    - Code has been tested on Ubuntu 22.04.3 LTS and Ubuntu 20.04.6 LTS (dockerised)</p> <p>Python Version: 3.8 or above    - Code has been developed and tested on Python 3.8, 3.9, and 3.10 (note: using Python 3.11 may result in some issues)</p> <p>Anaconda/Miniconda Version: 23.1.0 or above    - Code has been developed and tested on Conda version 23.1.0</p> <p>RAM: 64GB or above    - Code has been developed and tested on machines with:     - 128GB of RAM (Supermicro) &amp;     - 504GB of RAM (Nvidia-DGX)</p> <p>CPU: Any modern AMD/Intel Processors    - Code has been developed and tested on two different machines with:        - <code>AMD EPYC 7513 32-Core Processor</code> (Supermicro) &amp;        - <code>Intel(R) Xeon(R) CPU E5-2697A v4 @ 2.60GHz</code> (Nvidia-DGX)</p> <p>GPU: Latest cards with minimum 12GB GPU memory    - Code has been developed and tested on machines with: <code>RTX 3090 24GB</code> (Supermicro) and <code>Nvidia Titan XP 12GB</code> x 8 cards (Nvidia-DGX)</p>"},{"location":"modules/local-shape-descriptors/system_requirements/#databases","title":"Databases","text":"<p>MongoDB : For multi-worker inference, please install <code>MongoDB &lt;https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-ubuntu/#std-label-install-mdb-community-ubuntu&gt;</code>_. Our code has been tested with MongoDB <code>v6.0.5</code>.</p> <p>Postgres: We are currently extending support to <code>PostGreSQL &lt;https://www.postgresql.org/download/linux/ubuntu/&gt;</code>_. Our preliminary tests have been conducted on PostgreSQL <code>v14.9</code>.</p>"},{"location":"modules/local-shape-descriptors/system_requirements/#containerisation-of-application-environments","title":"Containerisation of application environments","text":"<p>| Our development and test environments have been containerised using dockers and apptainers.  | We highly recommend installing <code>Docker &lt;https://www.docker.com/get-started&gt;</code>_ : v24.0.6 or above. </p> <p>| <code>Apptainers (formerly Singularity) &lt;https://apptainer.org/docs/admin/main/installation.html#&gt;</code>_ can be your choice of containerisation too.  | Our apptainers have docker images as bases, please build them using the provided <code>.def</code> files. | We have tested the builds and runs on versions 1.2.3.</p>"},{"location":"modules/mitochondria-segmentation/","title":"Mitochondria Segmentation","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/mitochondria-segmentation/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/mitochondria-segmentation/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/mitochondria-segmentation/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/mitochondria-segmentation/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/mitochondria-segmentation/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/mitochondria-segmentation/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p>"},{"location":"modules/mitochondria-segmentation/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/mitochondria-segmentation/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"},{"location":"modules/neurotransmitter-classification/","title":"Neurotransmitter Classification","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/neurotransmitter-classification/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/neurotransmitter-classification/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/neurotransmitter-classification/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/neurotransmitter-classification/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/neurotransmitter-classification/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/neurotransmitter-classification/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p>"},{"location":"modules/neurotransmitter-classification/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/neurotransmitter-classification/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"},{"location":"modules/proofreading/","title":"Proofreading","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/proofreading/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/proofreading/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/proofreading/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/proofreading/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/proofreading/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/proofreading/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p>"},{"location":"modules/proofreading/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/proofreading/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"},{"location":"modules/simpsyn/","title":"Synful","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/simpsyn/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/simpsyn/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/simpsyn/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/simpsyn/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/simpsyn/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/simpsyn/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p>"},{"location":"modules/simpsyn/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/simpsyn/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"},{"location":"modules/synful/","title":"Synful","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts in GitHub to get more up-to-date information.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/synful/#what-is-synapse-detection","title":"What is synapse detection?","text":"<p>In connectome mapping, tracing out every neuron is not enough. We must know who are these neurons talking to, i.e. which other neurons.</p>"},{"location":"modules/synful/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/synful/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/synful/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/synful/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/synful/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/synful/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p>"},{"location":"modules/synful/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/synful/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"},{"location":"modules/visualize/","title":"Visualize","text":"<p>Short overview of what this module does and links to usage.</p> <ul> <li>Install &amp; Usage: See the module's README and scripts.</li> <li>Design Choices: See Design Choices for the \"why\" behind the \"what\".</li> </ul>"},{"location":"modules/visualize/design-choices/","title":"Design Choices","text":"<p>This page explains the decisions we made, alternatives considered, trade-offs, and future work.</p>"},{"location":"modules/visualize/design-choices/#problem-goal","title":"Problem / Goal","text":"<p>What outcome are we optimizing for?</p>"},{"location":"modules/visualize/design-choices/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Option A \u2013 pros/cons</li> <li>Option B \u2013 pros/cons</li> <li>Option C \u2013 pros/cons</li> </ul>"},{"location":"modules/visualize/design-choices/#decision","title":"Decision","text":"<p>What we chose and why.</p>"},{"location":"modules/visualize/design-choices/#trade-offs","title":"Trade-offs","text":"<p>Compute, memory, DX, accuracy, portability, etc.</p>"},{"location":"modules/visualize/design-choices/#implementation-notes","title":"Implementation Notes","text":"<p>Key parameters, scripts, data flow.</p>"},{"location":"modules/visualize/design-choices/#operational-guidance","title":"Operational Guidance","text":"<p>Tuning tips, failure modes, troubleshooting.</p>"},{"location":"modules/visualize/design-choices/#future-work-open-questions","title":"Future Work / Open Questions","text":"<p>What we plan to revisit and why.</p>"}]}